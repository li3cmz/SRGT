不要的数据:
train_supervised_large_y_filter_line_number=[223, 242, 316, 325, 444, 460, 474, 626, 674, 1054, 1152, 1232, 1275, 1352, 1414, 1454, 1483, 1560, 1747, 1792, 1800, 1829, 1838, 2289, 2316, 2383, 2450, 2463, 2524, 2571, 2909, 2961, 3075, 3189, 3333, 3500, 3803, 4051, 4084, 4112, 4134, 4396, 4436, 4497, 5195, 5386, 5407, 5456, 5691, 5957, 6657, 6736, 6741, 6771, 6817, 6836, 6980, 7369, 7460, 7463, 7481, 7500, 8135, 8169, 8200, 9011, 9166, 9568, 9710, 9847, 9931, 10429, 10455, 10758, 11016, 11086, 11135, 11223, 11308, 11325, 11385, 11702, 12079, 12978, 13007, 13165, 13222, 13248, 13254, 13283, 13327, 13391, 13403, 13440, 13527, 13535, 13578, 13907, 13938, 13959, 13985, 14118, 14203, 14221, 14503, 14612, 14677, 14760, 14797, 14866, 14877, 15001, 15050, 15089, 15102, 15254, 15355, 15588, 15696, 15753, 15772, 15898, 15924, 15962, 16008, 16072, 16094, 16109, 16143, 16209, 16234, 16369, 16370, 16502, 16610, 16706, 16777, 16958, 17308, 17362, 17366, 17453, 17601, 17788, 17843, 18014, 18190, 18252, 18343, 18450, 18454, 18507, 18606, 18693, 18725, 18729, 18762, 18788, 18802, 19035, 19103, 19133, 19186, 19222, 19262, 19431, 19614, 19879, 20777, 21402, 21910, 22043, 22453, 23123, 23957, 24027, 24281, 24449, 25119, 25651, 25957, 26033, 26101, 26388, 26847, 26904, 26964, 27136, 27243, 27261, 27501, 27637, 27737, 27846, 28089, 28190]
train_supervised_small_y_filter_line_number=[223, 242, 316, 325, 444, 460, 474, 626, 674, 1054, 1152, 1232, 1275, 1352, 1414, 1454, 1483, 1560, 1747, 1792, 1800, 1829, 1838, 2289, 2316, 2383, 2450, 2463, 2524, 2571, 2909, 2961, 3075, 3189, 3333, 3500, 3803, 4051, 4084, 4112, 4134, 4216, 4221, 4251, 4297, 4316, 4460, 4849, 4940, 4943, 4961, 4980, 5145, 5403, 5473, 5522, 5610, 5695, 5712, 5772, 6089, 6466, 7365, 7394, 7552, 7609, 7635, 7641, 7670, 7714, 7778, 7790, 7827, 7914, 7922, 7965, 8294, 8325, 8346, 8372, 8505, 8590, 8608, 8890, 8999, 9064, 9147, 9184, 9253, 9264, 9388, 9437, 9476, 9489, 9641, 9742, 9975, 10084, 10141, 10160, 10286, 10312, 10350, 10396, 10460, 10482, 10497, 10531, 10597, 10622, 10757, 10758, 10890, 10998, 11094, 11165, 11346, 11696, 11750, 11754, 11841, 11990, 12177, 12232, 12403, 12579, 12641, 12732, 12839, 12843, 12896, 12995, 13082, 13114, 13118, 13151, 13177, 13191, 13424, 13492, 13522, 13575, 13611, 13651, 13820, 14003, 14268, 15166, 15791, 16299, 16432]
train_unsupervised_y_filter_line_number=[]
test_data_y_filter_line_number=[67, 138, 154, 366, 388, 623, 799, 1121, 1184, 1209, 1279, 1297, 1375, 1467, 1549, 1554, 1716]
dev_data_y_filter_line_number= [103, 136, 290, 315, 358, 416, 614, 740, 894, 1088, 1129, 1445, 1463, 1690, 1828]

备注：
对于edit_endings，有3种结果，只取了第一种

数据处理步骤：
1. 先分别生成x1.txt, x2.txt, xx2.txt, y.txt, yy.txt
2. 手动用|||给y.txt分隔句子  ? ! .
3. 存下过滤origin中没分隔的句子，不打到3个的句子的line
4. 重新生成x1.txt, x2.txt, xx2.txt, y.txt, yy.txt
5. check yy1.txt yy2.txt yy3.txt行数是否足够
6. 将标点符号和单词分开 [, . ? ! '] 决定用代码处理
7. 提取train.text val.text和test.text来提取各自的vocab，这里也要做标点符号和单词的分开 =
8. 将新文件名中的_1去掉
9. python utils_preproc/stanford_dependency.py
    python utils_preproc/stanford_dependency.py data/TimeTravel/pre/test_data_y1.txt data/TimeTravel/pre/test_data_y1.adjs 9000
    python utils_preproc/dataset_read_dirt.py data/TimeTravel/pre/test_data_y1.adjs data/TimeTravel/TimeTravel.test_data_y1_adjs_dirt.tfrecords
    python utils_preproc/dataset_read.py data/TimeTravel/pre/test_data_y1.adjs data/TimeTravel/TimeTravel.test_data_y1_adjs_undirt.tfrecords


    python utils_preproc/stanford_dependency.py data/TimeTravel/pre/test_data_yy1.txt data/TimeTravel/pre/test_data_yy1.adjs 9000
    python utils_preproc/dataset_read_dirt.py data/TimeTravel/pre/test_data_yy1.adjs data/TimeTravel/TimeTravel.test_data_yy1_adjs_dirt.tfrecords
    python utils_preproc/dataset_read.py data/TimeTravel/pre/test_data_yy1.adjs data/TimeTravel/TimeTravel.test_data_yy1_adjs_undirt.tfrecords